{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('/Users/randy/Documents/GitHub/Twitch_Chat_Harassment/toxic_comment_classification/jigsaw-toxic-comment-classification-challenge/cleaned_train.csv').fillna(' ')\n",
    "test = pd.read_csv('/Users/randy/Documents/GitHub/Twitch_Chat_Harassment/toxic_comment_classification/jigsaw-toxic-comment-classification-challenge/cleaned_test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-ascii characters\n",
    "def remove_non_ascii(text):\n",
    "    text  = \"\".join([char for char in text if char in string.printable])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(lambda x: remove_non_ascii(x))\n",
    "test['comment_text'] = test['comment_text'].apply(lambda x: remove_non_ascii(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train['comment_text']\n",
    "list_sentences_test = test['comment_text']\n",
    "all_text = pd.concat([list_sentences_train, list_sentences_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6              cocksucker before you piss around on my work\n",
       "12        hey what is it @ | talk what is it an exclusiv...\n",
       "16        bye ! do not look come or think of comming bac...\n",
       "42        you are gay or antisemmitian ? archangel white...\n",
       "43                 fuck your filthy mother in the ass dry !\n",
       "44        i am sorry i am sorry i screwed around with so...\n",
       "51        get fucked up get fuckeeed up got a drink that...\n",
       "55        stupid peace of shit stop deleting my stuff as...\n",
       "56        = tony sidaway is obviously a fistfuckee he lo...\n",
       "58        my band page deletion you thought i was gone d...\n",
       "59        why cannot you believe how fat artie is ? did ...\n",
       "65        all of my edits are good cunts like you who re...\n",
       "79        hi ! i am back again ! last warning ! stop und...\n",
       "86        would you both shut up you do not run wikipedi...\n",
       "105             a pair of jew - hating weiner nazi schmucks\n",
       "151       sorry puck but no one ever said dick was numbe...\n",
       "159       unblock me or i will get my lawyers on to you ...\n",
       "168       you should be fired you are a moronic wimp who...\n",
       "176       i think that your a fagget get a oife and burn...\n",
       "181       you are a stupid fuck and your mother cunt stinks\n",
       "201       your blatant pov pushing neither of you guys h...\n",
       "206       give me a permanat block raseac ! ! ! remember...\n",
       "211                    fuck you block me you faggot pussy !\n",
       "218       kill all niggers i have hard that others have ...\n",
       "231       burn deck if that would guy burn deck is like ...\n",
       "238       u r a tw@ fuck off u gay boy u r smelly fuck u...\n",
       "268                            atheism is full of bias shit\n",
       "278       hey why you are spreading misconceptions and t...\n",
       "286       and you are ? let me know when you have crawle...\n",
       "295       this user is such a worthless goddamn faggot f...\n",
       "                                ...                        \n",
       "159253    what do you mean why do not you keep your nose...\n",
       "159268    hi bading putang ina mong bakla ka fuck you ka...\n",
       "159274    study some linguistics before you say somethin...\n",
       "159281    lol ! ! you are gay ! ! you will never know ho...\n",
       "159290    hey alabamoy boy why dont you stick your head ...\n",
       "159298               are you dumber than you look ? asshole\n",
       "159312    walter mercado antonio quite frankly you are a...\n",
       "159315                               - public interest crap\n",
       "159334    horse ass seriously dude what is that horse as...\n",
       "159336    oh fuck off the pansy jew would just whine abo...\n",
       "159342    fuck off turd do not ever ban me again you cun...\n",
       "159358    goethean and me i would like you to know i hav...\n",
       "159368    palin satan wow what a surprise ! the one homo...\n",
       "159378    go ahead and fucking ban me ~ like that will h...\n",
       "159382                         shut up you goddamn assclown\n",
       "159386    stop telling lies and trying to promote your p...\n",
       "159394                         your boring and retarded two\n",
       "159398    why did that idiot revert the reversion i made...\n",
       "159400    shalom semite get the fuck out of here i will ...\n",
       "159411    fat piece of shit you obese piece of shit i th...\n",
       "159423    ps : you are all middle - aged losers at home ...\n",
       "159448                                   yeah i no it sucks\n",
       "159449                        i think he is a gay fag ! ! !\n",
       "159478    thank you given the misuse of tools here and t...\n",
       "159493                                fucking faggot lolwat\n",
       "159494    our previous conversation you fucking shit eat...\n",
       "159514                    you are a mischievious pubic hair\n",
       "159541    your absurd edits your absurd edits on great w...\n",
       "159546    hey listen do not you ever ! ! ! ! delete my e...\n",
       "159554    and i am going to keep posting the stuff u del...\n",
       "Name: comment_text, Length: 15294, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic = list_sentences_train.loc[train['toxic'] == 1]\n",
    "toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = list_sentences_train.loc[train['toxic'] == 1]\n",
    "severe_toxic = list_sentences_train.loc[train['toxic'] == 1]\n",
    "obscene = list_sentences_train.loc[train['toxic'] == 1]\n",
    "threat = list_sentences_train.loc[train['toxic'] == 1]\n",
    "insult = list_sentences_train.loc[train['toxic'] == 1]\n",
    "identity_hate = list_sentences_train.loc[train['toxic'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cl_path = '/Users/randy/Documents/GitHub/Twitch_Chat_Harassment/toxic_comment_classification/jigsaw-toxic-comment-classification-challenge/cleanwords.txt'\n",
    "clean_word_dict = {}\n",
    "with open(cl_path, 'r', encoding='utf-8') as cl:\n",
    "    for line in cl:\n",
    "        line = line.strip('\\n')\n",
    "        typo, correct = line.split(',')\n",
    "        clean_word_dict[typo] = correct\n",
    "\n",
    "def clean_word(text):\n",
    "    replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "    special_character_removal = re.compile(r'[^a-z\\d ]', re.IGNORECASE)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
    "\n",
    "    for typo, correct in clean_word_dict.items():\n",
    "        text = re.sub(typo, \" \" + correct + \" \", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"iâ€™m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = replace_numbers.sub('', text)\n",
    "    return text\n",
    "\n",
    "train_text = []\n",
    "test_text = []\n",
    "toxic_text = []\n",
    "severe_toxic_text = []\n",
    "obscene_text = []\n",
    "threat_text = []\n",
    "insult_text = []\n",
    "identity_hate_text = []\n",
    "\n",
    "for text in list_sentences_train:\n",
    "    train_text.append(clean_word(text))\n",
    "    \n",
    "for text in list_sentences_test:\n",
    "    test_text.append(clean_word(text))\n",
    "    \n",
    "for text in toxic:\n",
    "    toxic_text.append(clean_word(text))\n",
    "\n",
    "for text in severe_toxic_text:\n",
    "    severe_toxic_text.append(clean_word(text))\n",
    "\n",
    "for text in obscene_text:\n",
    "    obscene_text.append(clean_word(text))\n",
    "\n",
    "for text in threat:\n",
    "    threat_text.append(clean_word(text))\n",
    "\n",
    "for text in insult:\n",
    "    insult_text.append(clean_word(text))\n",
    "\n",
    "for text in identity_hate:\n",
    "    identity_hate_text.append(clean_word(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for EFC\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "word_vectorizer.fit(all_text)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for logit\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "char_vectorizer.fit(all_text)\n",
    "\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply CountVectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for logit\n",
    "count_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "count_vec_fit = count_vectorizer.fit(all_text)\n",
    "\n",
    "train_count_features = count_vectorizer.transform(train_text)\n",
    "test_count_features = count_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count_features.toarray().sum(axis=0)\n",
    "count_df = pd.DataFrame(count_vec_fit.get_feature_names())\n",
    "count_df['counts'] = train_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizers (for each individual topic/feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toxic\n",
    "toxic_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "toxic_vec_fit = toxic_vectorizer.fit(all_text)\n",
    "\n",
    "toxic_count_features = toxic_vectorizer.transform(toxic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_count_df = pd.DataFrame(toxic_vec_fit.get_feature_names())\n",
    "toxic_count_df['counts'] = toxic_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#severe_toxic\n",
    "severe_toxic_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "severe_toxic_vec_fit = severe_toxic_vectorizer.fit(all_text)\n",
    "\n",
    "severe_toxic_count_features = severe_toxic_vectorizer.transform(severe_toxic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severe_toxic_count_df = pd.DataFrame(severe_toxic_vec_fit.get_feature_names())\n",
    "severe_toxic_count_df['counts'] = severe_toxic_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obscene\n",
    "obscene_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "obscene_vec_fit = obscene_vectorizer.fit(all_text)\n",
    "\n",
    "obscene_count_features = obscene_vectorizer.transform(obscene_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obscene_count_df = pd.DataFrame(obscene_vec_fit.get_feature_names())\n",
    "obscene_count_df['counts'] = obscene_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threat\n",
    "threat_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "threat_vec_fit = threat_vectorizer.fit(all_text)\n",
    "\n",
    "threat_count_features = threat_vectorizer.transform(threat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threat_count_df = pd.DataFrame(threat_vec_fit.get_feature_names())\n",
    "threat_count_df['counts'] = threat_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insult\n",
    "insult_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "insult_vec_fit = insult_vectorizer.fit(all_text)\n",
    "\n",
    "insult_count_features = insult_vectorizer.transform(insult_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insult_count_df = pd.DataFrame(insult_vec_fit.get_feature_names())\n",
    "insult_count_df['counts'] = insult_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identity_hate\n",
    "identity_hate_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "identity_hate_vec_fit = identity_hate_vectorizer.fit(all_text)\n",
    "\n",
    "identity_hate_count_features = identity_hate_vectorizer.transform(identity_hate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_hate_count_df = pd.DataFrame(identity_hate_vec_fit.get_feature_names())\n",
    "identity_hate_count_df['counts'] = identity_hate_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_of_toxic_category(category_string):\n",
    "    '''category string must match train dataframe column name exactly ''' \n",
    "    filtered_sentences = list_sentences_train.loc[train[category_string] == 1]\n",
    "    category_vectorizer = CountVectorizer(\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        ngram_range=(1, 1),\n",
    "        stop_words='english',\n",
    "        max_features=30000)\n",
    "    category_vec_fit = category_vectorizer.fit(filtered_sentences)\n",
    "\n",
    "    category_count_features = category_vectorizer.transform(filtered_sentences)\n",
    "    category_count_df = pd.DataFrame(category_vec_fit.get_feature_names())\n",
    "    category_count_df['counts'] = category_count_features.toarray().sum(axis=0)\n",
    "    category_count_df.rename(columns = {0: 'word'})\n",
    "    return category_count_df, filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_specific_vocab_dict = dict()\n",
    "class_specific_sentences = dict()\n",
    "for class_name in class_names:\n",
    "    class_specific_vocab_dict[class_name] = \\\n",
    "        create_df_of_toxic_category(class_name)[0].sort_values('counts', ascending = False)\n",
    "    class_specific_sentences[class_name] =\\\n",
    "            create_df_of_toxic_category(class_name)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_string = ''\n",
    "for line in class_specific_sentences['toxic']:\n",
    "    toxic_string+=' ' + line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(word_vectorizer.transform([toxic_string]).toarray()[0] == word_vectorizer.transform([toxic_string]).toarray().max()).index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer.get_feature_names()[10704]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "toxic_sentences_transformed = word_vectorizer.transform(class_specific_sentences['toxic']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_sentences_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_sentences_transformed.sum(axis=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_specific_vocab_dict['identity_hate'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toxic_df = create_df_of_toxic_category('toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_df.sort_values('counts',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_df.rename(columns = {0: 'word'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is kept for heroku purposes\n",
    "train_features = train_word_features\n",
    "test_features = test_word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on toxic set: 0.96202\n",
      "CV score for class toxic is 0.9702816984672603\n",
      "Accuracy of logistic regression classifier on severe_toxic set: 0.99104\n",
      "CV score for class severe_toxic is 0.9857997215697615\n",
      "Accuracy of logistic regression classifier on obscene set: 0.98003\n",
      "CV score for class obscene is 0.9859225066889902\n",
      "Accuracy of logistic regression classifier on threat set: 0.99729\n",
      "CV score for class threat is 0.9823470849859306\n",
      "Accuracy of logistic regression classifier on insult set: 0.97318\n",
      "CV score for class insult is 0.9769461122942081\n",
      "Accuracy of logistic regression classifier on identity_hate set: 0.99240\n",
      "CV score for class identity_hate is 0.9749884558642069\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "log_predictions = {'id': test['id']}\n",
    "log_models = {}\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    log_classifier = LogisticRegression(solver='sag')\n",
    "    log_classifier.fit(train_features, train_target)\n",
    "    \n",
    "    print('Accuracy of logistic regression classifier on {} set: {:.5f}'.format(class_name,log_classifier.score(train_features, train_target)))\n",
    "    \n",
    "    cv_loss = np.mean(cross_val_score(log_classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "    \n",
    "    log_models[class_name] = log_classifier\n",
    "    log_predictions[class_name] = log_classifier.predict_proba(test_features)[:, 1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/randy/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function plot_roc_curve is deprecated; This will be removed in v0.5.0. Please use scikitplot.metrics.plot_roc instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-daff3e1f5c51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;31m# ground truth labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_predictions\u001b[0m\u001b[0;31m# predicted probabilities generated by sklearn classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mskplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_roc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_probas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scikitplot/metrics.py\u001b[0m in \u001b[0;36mplot_roc_curve\u001b[0;34m(y_true, y_probas, title, curves, ax, figsize, cmap, title_fontsize, text_fontsize)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0mroc_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         fpr[i], tpr[i], _ = roc_curve(y_true, probas[:, i],\n\u001b[0m\u001b[1;32m    258\u001b[0m                                       pos_label=classes[i])\n\u001b[1;32m    259\u001b[0m         \u001b[0mroc_auc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true = losses# ground truth labels\n",
    "y_probas = log_predictions# predicted probabilities generated by sklearn classifier\n",
    "skplt.metrics.plot_roc_curve(y_true, y_probas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle the models\n",
    "# Save Model as a pickle Using joblib\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(log_models, 'Logistic_Regression_models.p')\n",
    "pickle.dump(train_char_features, open(\"train_char_features_vectorizer.p\", \"wb\"))\n",
    "pickle.dump(test_char_features, open(\"test_char_features_vectorizer.p\", \"wb\"))\n",
    "pickle.dump(word_vectorizer.fit(all_text), open(\"log_word_vectorizer.p\", \"wb\"))\n",
    "\n",
    "  \n",
    "# Load the model from the file \n",
    "# pickled_models = joblib.load('models.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "losses = []\n",
    "etc_predictions = {'id': test['id']}\n",
    "etc_models = {}\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    etc_classifier = ExtraTreesClassifier(n_estimators=30)\n",
    "    \n",
    "    cv_loss = np.mean(cross_val_score(etc_classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "    \n",
    "    etc_classifier.fit(train_features, train_target)\n",
    "    etc_models[class_name] = etc_classifier\n",
    "    etc_predictions[class_name] = etc_classifier.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle the models\n",
    "# Save Model as a pickle Using joblib\n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(etc_models, 'etc_models.p') \n",
    "  \n",
    "# Load the model from the file \n",
    "pickled_models = joblib.load('etc_models.p')  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_models['toxic'].fit(train_features, train_target)\n",
    "predictions['toxic'] = pickled_models['toxic'].predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = pickled_models['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_loss = np.mean(cross_val_score(toxic, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "print('CV score for toxic class is {}'.format(cv_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Code (Unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _train_model(train_x, test_features):\n",
    "#     predictions = {'id': test['id']}\n",
    "#     for class_name in class_names:\n",
    "#         train_target = train[class_name]\n",
    "#         classifier = LogisticRegression(solver='sag')\n",
    "#         classifier.fit(train_X, train_y)\n",
    "#         predictions[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "#     return predictions\n",
    "\n",
    "# def train_folds(X, y, fold_count, test_features):\n",
    "#     fold_size = len(X) // fold_count\n",
    "#     all_predections = []\n",
    "#     for fold_id in range(0, fold_count):\n",
    "#         fold_start = fold_size * fold_id\n",
    "#         fold_end = fold_start + fold_size\n",
    "\n",
    "#         if fold_id == fold_size - 1:\n",
    "#             fold_end = len(X)\n",
    "\n",
    "#         train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "#         train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "#         val_x = X[fold_start:fold_end]\n",
    "#         val_y = y[fold_start:fold_end]\n",
    "    \n",
    "#         print(\"In fold #\", fold_id)\n",
    "#         all_predections.append(_train_model(train_x, train_y))\n",
    "#     return all_predections\n",
    "\n",
    "# train_folds(train_features, test_features, train_features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# submission = pd.DataFrame.from_dict(predictions)\n",
    "# submission.to_csv('Logistic-Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup nltk corpora path and Google Word2Vec location\n",
    "google_vec_file = '/Users/randy/Documents/GitHub/Twitch_Chat_Harassment/notebooks/GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('king' ,topn=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_similarity(['king', 'man'], ['queen', 'woman'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
