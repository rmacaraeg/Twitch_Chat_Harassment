{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from scipy.sparse import hstack\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('/Users/randy/Documents/GitHub/Twitch_Chat_Harassment/toxic_comment_classification/jigsaw-toxic-comment-classification-challenge/cleaned_train.csv').fillna(' ')\n",
    "test = pd.read_csv('/Users/randy/Documents/GitHub/Twitch_Chat_Harassment/toxic_comment_classification/jigsaw-toxic-comment-classification-challenge/cleaned_test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-ascii characters\n",
    "def remove_non_ascii(text):\n",
    "    text  = \"\".join([char for char in text if char in string.printable])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(lambda x: remove_non_ascii(x))\n",
    "test['comment_text'] = test['comment_text'].apply(lambda x: remove_non_ascii(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train['comment_text']\n",
    "list_sentences_test = test['comment_text']\n",
    "all_text = pd.concat([list_sentences_train, list_sentences_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6              cocksucker before you piss around on my work\n",
       "12        hey what is it @ | talk what is it an exclusiv...\n",
       "16        bye ! do not look come or think of comming bac...\n",
       "42        you are gay or antisemmitian ? archangel white...\n",
       "43                 fuck your filthy mother in the ass dry !\n",
       "44        i am sorry i am sorry i screwed around with so...\n",
       "51        get fucked up get fuckeeed up got a drink that...\n",
       "55        stupid peace of shit stop deleting my stuff as...\n",
       "56        = tony sidaway is obviously a fistfuckee he lo...\n",
       "58        my band page deletion you thought i was gone d...\n",
       "59        why cannot you believe how fat artie is ? did ...\n",
       "65        all of my edits are good cunts like you who re...\n",
       "79        hi ! i am back again ! last warning ! stop und...\n",
       "86        would you both shut up you do not run wikipedi...\n",
       "105             a pair of jew - hating weiner nazi schmucks\n",
       "151       sorry puck but no one ever said dick was numbe...\n",
       "159       unblock me or i will get my lawyers on to you ...\n",
       "168       you should be fired you are a moronic wimp who...\n",
       "176       i think that your a fagget get a oife and burn...\n",
       "181       you are a stupid fuck and your mother cunt stinks\n",
       "201       your blatant pov pushing neither of you guys h...\n",
       "206       give me a permanat block raseac ! ! ! remember...\n",
       "211                    fuck you block me you faggot pussy !\n",
       "218       kill all niggers i have hard that others have ...\n",
       "231       burn deck if that would guy burn deck is like ...\n",
       "238       u r a tw@ fuck off u gay boy u r smelly fuck u...\n",
       "268                            atheism is full of bias shit\n",
       "278       hey why you are spreading misconceptions and t...\n",
       "286       and you are ? let me know when you have crawle...\n",
       "295       this user is such a worthless goddamn faggot f...\n",
       "                                ...                        \n",
       "159253    what do you mean why do not you keep your nose...\n",
       "159268    hi bading putang ina mong bakla ka fuck you ka...\n",
       "159274    study some linguistics before you say somethin...\n",
       "159281    lol ! ! you are gay ! ! you will never know ho...\n",
       "159290    hey alabamoy boy why dont you stick your head ...\n",
       "159298               are you dumber than you look ? asshole\n",
       "159312    walter mercado antonio quite frankly you are a...\n",
       "159315                               - public interest crap\n",
       "159334    horse ass seriously dude what is that horse as...\n",
       "159336    oh fuck off the pansy jew would just whine abo...\n",
       "159342    fuck off turd do not ever ban me again you cun...\n",
       "159358    goethean and me i would like you to know i hav...\n",
       "159368    palin satan wow what a surprise ! the one homo...\n",
       "159378    go ahead and fucking ban me ~ like that will h...\n",
       "159382                         shut up you goddamn assclown\n",
       "159386    stop telling lies and trying to promote your p...\n",
       "159394                         your boring and retarded two\n",
       "159398    why did that idiot revert the reversion i made...\n",
       "159400    shalom semite get the fuck out of here i will ...\n",
       "159411    fat piece of shit you obese piece of shit i th...\n",
       "159423    ps : you are all middle - aged losers at home ...\n",
       "159448                                   yeah i no it sucks\n",
       "159449                        i think he is a gay fag ! ! !\n",
       "159478    thank you given the misuse of tools here and t...\n",
       "159493                                fucking faggot lolwat\n",
       "159494    our previous conversation you fucking shit eat...\n",
       "159514                    you are a mischievious pubic hair\n",
       "159541    your absurd edits your absurd edits on great w...\n",
       "159546    hey listen do not you ever ! ! ! ! delete my e...\n",
       "159554    and i am going to keep posting the stuff u del...\n",
       "Name: comment_text, Length: 15294, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic = list_sentences_train.loc[train['toxic'] == 1]\n",
    "toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = list_sentences_train.loc[train['toxic'] == 1]\n",
    "severe_toxic = list_sentences_train.loc[train['toxic'] == 1]\n",
    "obscene = list_sentences_train.loc[train['toxic'] == 1]\n",
    "threat = list_sentences_train.loc[train['toxic'] == 1]\n",
    "insult = list_sentences_train.loc[train['toxic'] == 1]\n",
    "identity_hate = list_sentences_train.loc[train['toxic'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cl_path = '/Users/randy/Documents/GitHub/Twitch_Chat_Harassment/toxic_comment_classification/jigsaw-toxic-comment-classification-challenge/cleanwords.txt'\n",
    "clean_word_dict = {}\n",
    "with open(cl_path, 'r', encoding='utf-8') as cl:\n",
    "    for line in cl:\n",
    "        line = line.strip('\\n')\n",
    "        typo, correct = line.split(',')\n",
    "        clean_word_dict[typo] = correct\n",
    "\n",
    "def clean_word(text):\n",
    "    replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "    special_character_removal = re.compile(r'[^a-z\\d ]', re.IGNORECASE)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
    "\n",
    "    for typo, correct in clean_word_dict.items():\n",
    "        text = re.sub(typo, \" \" + correct + \" \", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"iâ€™m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = replace_numbers.sub('', text)\n",
    "    return text\n",
    "\n",
    "train_text = []\n",
    "test_text = []\n",
    "toxic_text = []\n",
    "severe_toxic_text = []\n",
    "obscene_text = []\n",
    "threat_text = []\n",
    "insult_text = []\n",
    "identity_hate_text = []\n",
    "\n",
    "for text in list_sentences_train:\n",
    "    train_text.append(clean_word(text))\n",
    "    \n",
    "for text in list_sentences_test:\n",
    "    test_text.append(clean_word(text))\n",
    "    \n",
    "for text in toxic:\n",
    "    toxic_text.append(clean_word(text))\n",
    "\n",
    "for text in severe_toxic_text:\n",
    "    severe_toxic_text.append(clean_word(text))\n",
    "\n",
    "for text in obscene_text:\n",
    "    obscene_text.append(clean_word(text))\n",
    "\n",
    "for text in threat:\n",
    "    threat_text.append(clean_word(text))\n",
    "\n",
    "for text in insult:\n",
    "    insult_text.append(clean_word(text))\n",
    "\n",
    "for text in identity_hate:\n",
    "    identity_hate_text.append(clean_word(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for EFC\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "word_vectorizer.fit(all_text)\n",
    "\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for logit\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "char_vectorizer.fit(all_text)\n",
    "\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply CountVectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for logit\n",
    "count_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "count_vec_fit = count_vectorizer.fit(all_text)\n",
    "\n",
    "train_count_features = count_vectorizer.transform(train_text)\n",
    "test_count_features = count_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count_features.toarray().sum(axis=0)\n",
    "count_df = pd.DataFrame(count_vec_fit.get_feature_names())\n",
    "count_df['counts'] = train_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizers (for each individual topic/feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toxic\n",
    "toxic_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "toxic_vec_fit = toxic_vectorizer.fit(all_text)\n",
    "\n",
    "toxic_count_features = toxic_vectorizer.transform(toxic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_count_df = pd.DataFrame(toxic_vec_fit.get_feature_names())\n",
    "toxic_count_df['counts'] = toxic_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#severe_toxic\n",
    "severe_toxic_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "severe_toxic_vec_fit = severe_toxic_vectorizer.fit(all_text)\n",
    "\n",
    "severe_toxic_count_features = severe_toxic_vectorizer.transform(severe_toxic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "severe_toxic_count_df = pd.DataFrame(severe_toxic_vec_fit.get_feature_names())\n",
    "severe_toxic_count_df['counts'] = severe_toxic_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obscene\n",
    "obscene_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "obscene_vec_fit = obscene_vectorizer.fit(all_text)\n",
    "\n",
    "obscene_count_features = obscene_vectorizer.transform(obscene_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "obscene_count_df = pd.DataFrame(obscene_vec_fit.get_feature_names())\n",
    "obscene_count_df['counts'] = obscene_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#threat\n",
    "threat_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "threat_vec_fit = threat_vectorizer.fit(all_text)\n",
    "\n",
    "threat_count_features = threat_vectorizer.transform(threat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "threat_count_df = pd.DataFrame(threat_vec_fit.get_feature_names())\n",
    "threat_count_df['counts'] = threat_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insult\n",
    "insult_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "insult_vec_fit = insult_vectorizer.fit(all_text)\n",
    "\n",
    "insult_count_features = insult_vectorizer.transform(insult_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "insult_count_df = pd.DataFrame(insult_vec_fit.get_feature_names())\n",
    "insult_count_df['counts'] = insult_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identity_hate\n",
    "identity_hate_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words='english',\n",
    "    max_features=30000)\n",
    "identity_hate_vec_fit = identity_hate_vectorizer.fit(all_text)\n",
    "\n",
    "identity_hate_count_features = identity_hate_vectorizer.transform(identity_hate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_hate_count_df = pd.DataFrame(identity_hate_vec_fit.get_feature_names())\n",
    "identity_hate_count_df['counts'] = identity_hate_count_features.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_of_toxic_category(category_string):\n",
    "    '''category string must match train dataframe column name exactly ''' \n",
    "    filtered_sentences = list_sentences_train.loc[train[category_string] == 1]\n",
    "    category_vectorizer = CountVectorizer(\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        ngram_range=(1, 1),\n",
    "        stop_words='english',\n",
    "        max_features=30000)\n",
    "    category_vec_fit = category_vectorizer.fit(filtered_sentences)\n",
    "\n",
    "    category_count_features = category_vectorizer.transform(filtered_sentences)\n",
    "    category_count_df = pd.DataFrame(category_vec_fit.get_feature_names())\n",
    "    category_count_df['counts'] = category_count_features.toarray().sum(axis=0)\n",
    "    category_count_df.rename(columns = {0: 'word'})\n",
    "    return category_count_df, filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_specific_vocab_dict = dict()\n",
    "class_specific_sentences = dict()\n",
    "for class_name in class_names:\n",
    "    class_specific_vocab_dict[class_name] = \\\n",
    "        create_df_of_toxic_category(class_name)[0].sort_values('counts', ascending = False)\n",
    "    class_specific_sentences[class_name] =\\\n",
    "            create_df_of_toxic_category(class_name)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_string = ''\n",
    "for line in class_specific_sentences['toxic']:\n",
    "    toxic_string+=' ' + line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__',\n",
       " '___',\n",
       " '____',\n",
       " '_____',\n",
       " '______',\n",
       " '__toc__',\n",
       " '_id',\n",
       " '_l',\n",
       " '_noticeboard',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aaaaaaaaaaaaaaaa',\n",
       " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       " 'aad',\n",
       " 'aah',\n",
       " 'aahahahahahahjaahahahahahahaahh',\n",
       " 'aaliyah',\n",
       " 'aan',\n",
       " 'aap',\n",
       " 'aardvark',\n",
       " 'aaron',\n",
       " 'aas',\n",
       " 'aave',\n",
       " 'ab',\n",
       " 'aba',\n",
       " 'aback',\n",
       " 'abad',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abba',\n",
       " 'abbas',\n",
       " 'abbey',\n",
       " 'abbot',\n",
       " 'abbott',\n",
       " 'abbrev',\n",
       " 'abbreviate',\n",
       " 'abbreviated',\n",
       " 'abbreviation',\n",
       " 'abbreviations',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abd',\n",
       " 'abdel',\n",
       " 'abdelkader',\n",
       " 'abducted',\n",
       " 'abduction',\n",
       " 'abdul',\n",
       " 'abdullah',\n",
       " 'abe',\n",
       " 'abeed',\n",
       " 'abel',\n",
       " 'aber',\n",
       " 'abf',\n",
       " 'abhira',\n",
       " 'abhiras',\n",
       " 'abhishek',\n",
       " 'abhor',\n",
       " 'abhorrent',\n",
       " 'abi',\n",
       " 'abide',\n",
       " 'abiding',\n",
       " 'abigail',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abiogenesis',\n",
       " 'abit',\n",
       " 'abject',\n",
       " 'abkhazia',\n",
       " 'able',\n",
       " 'abnormal',\n",
       " 'aboard',\n",
       " 'abode',\n",
       " 'abolish',\n",
       " 'abolished',\n",
       " 'abolition',\n",
       " 'abolitionist',\n",
       " 'abomination',\n",
       " 'aboriginal',\n",
       " 'aborigines',\n",
       " 'aborted',\n",
       " 'abortion',\n",
       " 'abortions',\n",
       " 'abou',\n",
       " 'abound',\n",
       " 'aboutus',\n",
       " 'abraham',\n",
       " 'abrahamic',\n",
       " 'abramoff',\n",
       " 'abrams',\n",
       " 'abrasive',\n",
       " 'abridged',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolutly',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'absorption',\n",
       " 'abstain',\n",
       " 'abstaining',\n",
       " 'abstimmungsgegner',\n",
       " 'abstinence',\n",
       " 'abstract',\n",
       " 'abstraction',\n",
       " 'abstracts',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'abt',\n",
       " 'abtract',\n",
       " 'abu',\n",
       " 'abuja',\n",
       " 'abujihad',\n",
       " 'abul',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuser',\n",
       " 'abusers',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'abusively',\n",
       " 'abut',\n",
       " 'abysmal',\n",
       " 'abyss',\n",
       " 'ac',\n",
       " 'aca',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academical',\n",
       " 'academically',\n",
       " 'academics',\n",
       " 'academies',\n",
       " 'academy',\n",
       " 'acalamari',\n",
       " 'acc',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerating',\n",
       " 'acceleration',\n",
       " 'accelerations',\n",
       " 'accelerator',\n",
       " 'accent',\n",
       " 'accented',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'acceptability',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessable',\n",
       " 'accessdate',\n",
       " 'accessed',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accessing',\n",
       " 'accession',\n",
       " 'accessories',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidently',\n",
       " 'accidents',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'accolades',\n",
       " 'accommodate',\n",
       " 'accommodation',\n",
       " 'accomodate',\n",
       " 'accompanied',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplice',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accord',\n",
       " 'accordance',\n",
       " 'accorded',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accords',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountant',\n",
       " 'accountants',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accounts',\n",
       " 'accreditation',\n",
       " 'accredited',\n",
       " 'accross',\n",
       " 'accumulate',\n",
       " 'accumulated',\n",
       " 'accumulating',\n",
       " 'accumulation',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accusatory',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuser',\n",
       " 'accusers',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accussed',\n",
       " 'accustomed',\n",
       " 'ace',\n",
       " 'aces',\n",
       " 'ach',\n",
       " 'acharya',\n",
       " 'acheived',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'achilles',\n",
       " 'acid',\n",
       " 'acids',\n",
       " 'acim',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledgement',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'ackoz',\n",
       " 'aclu',\n",
       " 'acm',\n",
       " 'acn',\n",
       " 'aco',\n",
       " 'acording',\n",
       " 'acorn',\n",
       " 'acosta',\n",
       " 'acount',\n",
       " 'acoustic',\n",
       " 'acquaint',\n",
       " 'acquaintance',\n",
       " 'acquainted',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquiring',\n",
       " 'acquisition',\n",
       " 'acquitted',\n",
       " 'acr',\n",
       " 'acre',\n",
       " 'acreees',\n",
       " 'acres',\n",
       " 'acronym',\n",
       " 'acronyms',\n",
       " 'acroterion',\n",
       " 'acs',\n",
       " 'act',\n",
       " 'acta',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actionable',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activation',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activism',\n",
       " 'activist',\n",
       " 'activists',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actuality',\n",
       " 'actually',\n",
       " 'actualy',\n",
       " 'acu',\n",
       " 'acupuncture',\n",
       " 'acutally',\n",
       " 'acute',\n",
       " 'acw',\n",
       " 'ad',\n",
       " 'ada',\n",
       " 'adalah',\n",
       " 'adam',\n",
       " 'adamant',\n",
       " 'adams',\n",
       " 'adapt',\n",
       " 'adaptation',\n",
       " 'adaptations',\n",
       " 'adapted',\n",
       " 'adapting',\n",
       " 'adaptive',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addendum',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addictive',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addison',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additions',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'addy',\n",
       " 'adelaide',\n",
       " 'adele',\n",
       " 'adept',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhd',\n",
       " 'adhere',\n",
       " 'adhered',\n",
       " 'adherence',\n",
       " 'adherent',\n",
       " 'adherents',\n",
       " 'adheres',\n",
       " 'adhering',\n",
       " 'adi',\n",
       " 'adidas',\n",
       " 'adige',\n",
       " 'adil',\n",
       " 'adios',\n",
       " 'adj',\n",
       " 'adjacent',\n",
       " 'adjectival',\n",
       " 'adjective',\n",
       " 'adjectives',\n",
       " 'adjoining',\n",
       " 'adjunct',\n",
       " 'adjust',\n",
       " 'adjustable',\n",
       " 'adjusted',\n",
       " 'adjusting',\n",
       " 'adjustment',\n",
       " 'adjustments',\n",
       " 'adl',\n",
       " 'adler',\n",
       " 'adm',\n",
       " 'admi',\n",
       " 'admin',\n",
       " 'administer',\n",
       " 'administered',\n",
       " 'administering',\n",
       " 'administers',\n",
       " 'administration',\n",
       " 'administrations',\n",
       " 'administrative',\n",
       " 'administrator',\n",
       " 'administrators',\n",
       " 'admins',\n",
       " 'adminship',\n",
       " 'adminstrator',\n",
       " 'adminstrators',\n",
       " 'admirable',\n",
       " 'admirably',\n",
       " 'admiral',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admirer',\n",
       " 'admirers',\n",
       " 'admissible',\n",
       " 'admission',\n",
       " 'admissions',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admittance',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'admixture',\n",
       " 'admonished',\n",
       " 'adn',\n",
       " 'adnan',\n",
       " 'ado',\n",
       " 'adobe',\n",
       " 'adolescence',\n",
       " 'adolescent',\n",
       " 'adolescents',\n",
       " 'adolf',\n",
       " 'adolph',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adoptee',\n",
       " 'adopter',\n",
       " 'adopters',\n",
       " 'adopting',\n",
       " 'adoption',\n",
       " 'adopts',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'adorned',\n",
       " 'adrenaline',\n",
       " 'adress',\n",
       " 'adressed',\n",
       " 'adresses',\n",
       " 'adrian',\n",
       " 'adriatic',\n",
       " 'adrift',\n",
       " 'ads',\n",
       " 'adsydfiusagjfasfsduyaidfasgiudf',\n",
       " 'adult',\n",
       " 'adultery',\n",
       " 'adulthood',\n",
       " 'adults',\n",
       " 'adv',\n",
       " 'advaita',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advancement',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantageous',\n",
       " 'advantages',\n",
       " 'advent',\n",
       " 'adventist',\n",
       " 'adventure',\n",
       " 'adventures',\n",
       " 'adverb',\n",
       " 'adversarial',\n",
       " 'adversaries',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'adversely',\n",
       " 'advert',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advertisements',\n",
       " 'advertiser',\n",
       " 'advertisers',\n",
       " 'advertises',\n",
       " 'advertising',\n",
       " 'advertisment',\n",
       " 'adverts',\n",
       " 'advice',\n",
       " 'advices',\n",
       " 'advisable',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'adviser',\n",
       " 'advisers',\n",
       " 'advises',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'advisors',\n",
       " 'advisory',\n",
       " 'advocacy',\n",
       " 'advocate',\n",
       " 'advocated',\n",
       " 'advocates',\n",
       " 'advocating',\n",
       " 'ae',\n",
       " 'aegean',\n",
       " 'aeon',\n",
       " 'aerial',\n",
       " 'aero',\n",
       " 'aerodynamics',\n",
       " 'aeropagitica',\n",
       " 'aeroplane',\n",
       " 'aerosol',\n",
       " 'aerospace',\n",
       " 'aes',\n",
       " 'aesop',\n",
       " 'aesthetic',\n",
       " 'aesthetically',\n",
       " 'aesthetics',\n",
       " 'aether',\n",
       " 'aewaa',\n",
       " 'af',\n",
       " 'afa',\n",
       " 'afaict',\n",
       " 'afaik',\n",
       " 'afar',\n",
       " 'afc',\n",
       " 'afd',\n",
       " 'afds',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affection',\n",
       " 'affects',\n",
       " 'affidavit',\n",
       " 'affiliate',\n",
       " 'affiliated',\n",
       " 'affiliates',\n",
       " 'affiliation',\n",
       " 'affiliations',\n",
       " 'affinity',\n",
       " 'affirm',\n",
       " 'affirmation',\n",
       " 'affirmative',\n",
       " 'affirmed',\n",
       " 'affirming',\n",
       " 'affirms',\n",
       " 'affix',\n",
       " 'afflicted',\n",
       " 'affluent',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'afforded',\n",
       " 'affront',\n",
       " 'afghan',\n",
       " 'afghani',\n",
       " 'afghanistan',\n",
       " 'afghans',\n",
       " 'afghooni',\n",
       " 'afi',\n",
       " 'afic',\n",
       " 'afl',\n",
       " 'aforementioned',\n",
       " 'afoul',\n",
       " 'afp',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'africans',\n",
       " 'afrika',\n",
       " 'afrinic',\n",
       " 'afro',\n",
       " 'afrocentric',\n",
       " 'afrocentrism',\n",
       " 'aft',\n",
       " 'afterall',\n",
       " 'afterlife',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'afterthought',\n",
       " 'afterward',\n",
       " 'ag',\n",
       " 'aga',\n",
       " 'agains',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agendas',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'agf',\n",
       " 'aggie',\n",
       " 'aggiebean',\n",
       " 'aggrandizing',\n",
       " 'aggravated',\n",
       " 'aggravating',\n",
       " 'aggregate',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'aggressor',\n",
       " 'aggressors',\n",
       " 'aggrieved',\n",
       " 'agian',\n",
       " 'aging',\n",
       " 'agitated',\n",
       " 'agitation',\n",
       " 'agk',\n",
       " 'agnes',\n",
       " 'agnostic',\n",
       " 'agnosticism',\n",
       " 'agnostics',\n",
       " 'ago',\n",
       " 'agony',\n",
       " 'agree',\n",
       " 'agreeable',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agreements',\n",
       " 'agrees',\n",
       " 'agressive',\n",
       " 'agricultural',\n",
       " 'agriculture',\n",
       " 'aguante',\n",
       " 'aguilera',\n",
       " 'agw',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahahahahahahahaha',\n",
       " 'ahahahahahahahahahahahahahahahahahahaha',\n",
       " 'ahbash',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahhhhhhhhhhhha',\n",
       " 'ahir',\n",
       " 'ahistorical',\n",
       " 'ahle',\n",
       " 'ahmad',\n",
       " 'ahmadinejad',\n",
       " 'ahmadiyya',\n",
       " 'ahmed',\n",
       " 'ahole',\n",
       " 'ahot',\n",
       " 'ahoy',\n",
       " 'ahs',\n",
       " 'ahunt',\n",
       " 'ahve',\n",
       " 'ai',\n",
       " 'aid',\n",
       " 'aide',\n",
       " 'aided',\n",
       " 'aiden',\n",
       " 'aiding',\n",
       " 'aids',\n",
       " 'aidsaids',\n",
       " 'aig',\n",
       " 'aikido',\n",
       " 'aillrin',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aiming',\n",
       " 'aims',\n",
       " 'ain',\n",
       " 'aint',\n",
       " 'ainu',\n",
       " 'aipac',\n",
       " 'air',\n",
       " 'airbender',\n",
       " 'airborne',\n",
       " 'airbus',\n",
       " 'aircraft',\n",
       " 'airdate',\n",
       " 'airdates',\n",
       " 'aired',\n",
       " 'aires',\n",
       " 'airforce',\n",
       " 'airing',\n",
       " 'airline',\n",
       " 'airliner',\n",
       " 'airliners',\n",
       " 'airlines',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airplay',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airs',\n",
       " 'airship',\n",
       " 'airways',\n",
       " 'ait',\n",
       " 'aiv',\n",
       " 'aj',\n",
       " 'ajaltoun',\n",
       " 'ajax',\n",
       " 'ajay',\n",
       " 'ajith',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akan',\n",
       " 'akbar',\n",
       " 'akc',\n",
       " 'aki',\n",
       " 'akin',\n",
       " 'akins',\n",
       " 'akira',\n",
       " 'akkadian',\n",
       " 'ako',\n",
       " 'akon',\n",
       " 'aku',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'alabama',\n",
       " 'aladdin',\n",
       " 'alain',\n",
       " 'alamo',\n",
       " 'alan',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alarming',\n",
       " 'alas',\n",
       " 'alasdair',\n",
       " 'alaska',\n",
       " 'alaskan',\n",
       " 'alba',\n",
       " 'albania',\n",
       " 'albanian',\n",
       " 'albanians',\n",
       " 'albany',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'alberto',\n",
       " 'albino',\n",
       " 'albion',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'albuquerque',\n",
       " 'alchemist',\n",
       " 'alcohol',\n",
       " 'alcoholic',\n",
       " 'alcoholics',\n",
       " 'alcoholism',\n",
       " 'alden',\n",
       " 'aldux',\n",
       " 'ale',\n",
       " 'alec',\n",
       " 'aleem',\n",
       " 'alejandro',\n",
       " 'aleppo',\n",
       " 'alert',\n",
       " 'alerted',\n",
       " 'alerting',\n",
       " 'alerts',\n",
       " 'alessandro',\n",
       " 'alex',\n",
       " 'alexa',\n",
       " 'alexander',\n",
       " 'alexandra',\n",
       " 'alexandria',\n",
       " 'alexandrovich',\n",
       " 'alexikoua',\n",
       " 'alexis',\n",
       " 'alf',\n",
       " 'alfa',\n",
       " 'alfonso',\n",
       " 'alfred',\n",
       " 'algae',\n",
       " 'algebra',\n",
       " 'algebraic',\n",
       " 'algeria',\n",
       " 'algerian',\n",
       " 'algo',\n",
       " 'algorithm',\n",
       " 'algorithms',\n",
       " 'algrie',\n",
       " 'ali',\n",
       " 'alia',\n",
       " 'alias',\n",
       " 'aliases',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alien',\n",
       " 'alienate',\n",
       " 'alienated',\n",
       " 'aliens',\n",
       " 'align',\n",
       " 'aligned',\n",
       " 'alignment',\n",
       " 'alike',\n",
       " 'alison',\n",
       " 'alistair',\n",
       " 'alive',\n",
       " 'alkaline',\n",
       " 'alla',\n",
       " 'allah',\n",
       " 'allahu',\n",
       " 'allan',\n",
       " 'alle',\n",
       " 'alledged',\n",
       " 'allegation',\n",
       " 'allegations',\n",
       " 'allege',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'alleges',\n",
       " 'allegiance',\n",
       " 'alleging',\n",
       " 'allegory',\n",
       " 'allele',\n",
       " 'alleles',\n",
       " 'allemande',\n",
       " 'allen',\n",
       " 'allergic',\n",
       " 'allergies',\n",
       " 'allergy',\n",
       " 'alles',\n",
       " 'alleviate',\n",
       " 'alley',\n",
       " 'alliance',\n",
       " 'alliances',\n",
       " 'allied',\n",
       " 'allies',\n",
       " 'allison',\n",
       " 'alll',\n",
       " 'allmusic',\n",
       " 'allocated',\n",
       " 'allocation',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'allowable',\n",
       " 'allowance',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'alloy',\n",
       " 'allready',\n",
       " 'allright',\n",
       " 'alls',\n",
       " 'allude',\n",
       " 'alluded',\n",
       " 'alludes',\n",
       " 'alluding',\n",
       " 'allumungi',\n",
       " 'allusion',\n",
       " 'allusions',\n",
       " 'allways',\n",
       " 'ally',\n",
       " 'alma',\n",
       " 'almanac',\n",
       " 'almighty',\n",
       " 'alo',\n",
       " 'alongside',\n",
       " 'alonso',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'alp',\n",
       " 'alpaugh',\n",
       " 'alpha',\n",
       " 'alphabet',\n",
       " 'alphabetic',\n",
       " 'alphabetical',\n",
       " 'alphabetically',\n",
       " 'alphabetized',\n",
       " 'alphabets',\n",
       " 'alphonse',\n",
       " 'alpine',\n",
       " 'alps',\n",
       " 'alr',\n",
       " 'alredy',\n",
       " 'alright',\n",
       " 'als',\n",
       " 'alstair',\n",
       " 'alstom',\n",
       " 'alt',\n",
       " 'alta',\n",
       " 'altaic',\n",
       " 'altar',\n",
       " 'altenmann',\n",
       " 'alter',\n",
       " 'alteration',\n",
       " 'alterations',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'alternate',\n",
       " 'alternately',\n",
       " 'alternates',\n",
       " 'alternating',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'alternatives',\n",
       " 'alters',\n",
       " 'altetendekrabbe',\n",
       " 'altho',\n",
       " 'altitude',\n",
       " 'alto',\n",
       " 'altogether',\n",
       " 'altruistic',\n",
       " 'alum',\n",
       " 'aluminium',\n",
       " 'aluminum',\n",
       " 'alumni',\n",
       " 'alumnus',\n",
       " 'alums',\n",
       " 'alun',\n",
       " 'alva',\n",
       " 'alvarez',\n",
       " 'alveolar',\n",
       " 'alves',\n",
       " 'alvin',\n",
       " 'alzheimer',\n",
       " 'ama',\n",
       " 'amalgamation',\n",
       " 'amalthea',\n",
       " 'amanda',\n",
       " 'amar',\n",
       " 'amateur',\n",
       " 'amateurish',\n",
       " 'amateurs',\n",
       " 'amaury',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazes',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'amb',\n",
       " 'ambassador',\n",
       " 'ambassadors',\n",
       " 'ambedkar',\n",
       " 'amber',\n",
       " 'ambient',\n",
       " 'ambiguities',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambiguously',\n",
       " 'ambition',\n",
       " 'ambitions',\n",
       " 'ambitious',\n",
       " 'ambivalent',\n",
       " 'ambrose',\n",
       " 'ambulance',\n",
       " 'ambush',\n",
       " 'amc',\n",
       " 'amd',\n",
       " 'ame',\n",
       " 'amelia',\n",
       " 'amen',\n",
       " 'amenable',\n",
       " 'amend',\n",
       " 'amended',\n",
       " 'amending',\n",
       " 'amendment',\n",
       " 'amendments',\n",
       " 'amends',\n",
       " 'ameno',\n",
       " 'amer',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americanism',\n",
       " 'americans',\n",
       " 'americas',\n",
       " 'americorps',\n",
       " 'amerika',\n",
       " 'amerindian',\n",
       " 'ames',\n",
       " 'amg',\n",
       " 'ami',\n",
       " 'amib',\n",
       " 'amicable',\n",
       " 'amicably',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'amiga',\n",
       " 'amigo',\n",
       " 'amin',\n",
       " 'amino',\n",
       " 'amir',\n",
       " 'amirite',\n",
       " 'amish',\n",
       " 'amiss',\n",
       " 'amit',\n",
       " 'ammendment',\n",
       " 'ammo',\n",
       " 'ammonia',\n",
       " 'ammount',\n",
       " 'ammunition',\n",
       " 'amnesty',\n",
       " 'amok',\n",
       " 'amon',\n",
       " 'amor',\n",
       " 'amorphous',\n",
       " 'amortias',\n",
       " 'amos',\n",
       " 'amounted',\n",
       " 'amounting',\n",
       " 'amounts',\n",
       " 'amp',\n",
       " 'amphetamine',\n",
       " 'amphibious',\n",
       " 'ample',\n",
       " 'amplitude',\n",
       " 'amply',\n",
       " 'ams',\n",
       " 'amsterdam',\n",
       " 'amtrak',\n",
       " 'amuse',\n",
       " 'amused',\n",
       " ...]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10704"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_vectorizer.transform([toxic_string]).toarray()[0] == word_vectorizer.transform([toxic_string]).toarray().max()).index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fucksex'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer.get_feature_names()[10704]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "toxic_sentences_transformed = word_vectorizer.transform(class_specific_sentences['toxic']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_sentences_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743.4650972457656"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_sentences_transformed.sum(axis=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4226</th>\n",
       "      <td>nigger</td>\n",
       "      <td>2969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>fat</td>\n",
       "      <td>1322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>jew</td>\n",
       "      <td>1315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>gay</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>fuck</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  counts\n",
       "4226  nigger    2969\n",
       "2225     fat    1322\n",
       "3328     jew    1315\n",
       "2535     gay     918\n",
       "2443    fuck     880"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_specific_vocab_dict['identity_hate'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toxic_df = create_df_of_toxic_category('toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_df.sort_values('counts',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>___</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>____</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_____</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>______</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>__________________</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>__________________________</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>____________________________</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>_________________________________________________</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>______________________________________________...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>______________________________________________...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>_______________________two</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>__goosebumps</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>__o</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>_ccker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>_cking</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>_halfway</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>_holocaust_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>_ipatabi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>_is_</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>_l</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>_l_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>_sm</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>_upl</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>_war_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>a_hole</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>aa</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>aaa</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>aaaa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>aaaaa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29970</th>\n",
       "      <td>zod</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29971</th>\n",
       "      <td>zoe</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29972</th>\n",
       "      <td>zoey</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29973</th>\n",
       "      <td>zola</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29974</th>\n",
       "      <td>zoloft</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29975</th>\n",
       "      <td>zombie</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29976</th>\n",
       "      <td>zombies</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29977</th>\n",
       "      <td>zomfg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29978</th>\n",
       "      <td>zomg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29979</th>\n",
       "      <td>zone</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29980</th>\n",
       "      <td>zonked</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29981</th>\n",
       "      <td>zoo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29982</th>\n",
       "      <td>zoom</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29983</th>\n",
       "      <td>zoomin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29984</th>\n",
       "      <td>zoophilia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29985</th>\n",
       "      <td>zscout</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29986</th>\n",
       "      <td>zsero</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29987</th>\n",
       "      <td>zt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29988</th>\n",
       "      <td>zu</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29989</th>\n",
       "      <td>zubaty</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29990</th>\n",
       "      <td>zubrins</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29991</th>\n",
       "      <td>zuck</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29992</th>\n",
       "      <td>zuckerberg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29993</th>\n",
       "      <td>zuma</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29994</th>\n",
       "      <td>zurich</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>zus</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>zyklon</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>zzuuzz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>zzzz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>zzzzzzz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    word  counts\n",
       "0                                                     __       6\n",
       "1                                                    ___       5\n",
       "2                                                   ____       4\n",
       "3                                                  _____       1\n",
       "4                                                 ______       2\n",
       "5                                     __________________       1\n",
       "6                             __________________________       1\n",
       "7                           ____________________________       1\n",
       "8      _________________________________________________       1\n",
       "9      ______________________________________________...       1\n",
       "10     ______________________________________________...       1\n",
       "11                            _______________________two       1\n",
       "12                                          __goosebumps       1\n",
       "13                                                   __o       1\n",
       "14                                                _ccker       1\n",
       "15                                                _cking       1\n",
       "16                                              _halfway       1\n",
       "17                                           _holocaust_       1\n",
       "18                                              _ipatabi       1\n",
       "19                                                  _is_       2\n",
       "20                                                    _l       1\n",
       "21                                                   _l_       1\n",
       "22                                                   _sm       2\n",
       "23                                                  _upl       2\n",
       "24                                                 _war_       1\n",
       "25                                                a_hole       1\n",
       "26                                                    aa       4\n",
       "27                                                   aaa       3\n",
       "28                                                  aaaa       1\n",
       "29                                                 aaaaa       2\n",
       "...                                                  ...     ...\n",
       "29970                                                zod       3\n",
       "29971                                                zoe       7\n",
       "29972                                               zoey       3\n",
       "29973                                               zola       1\n",
       "29974                                             zoloft       1\n",
       "29975                                             zombie       3\n",
       "29976                                            zombies       4\n",
       "29977                                              zomfg       2\n",
       "29978                                               zomg       2\n",
       "29979                                               zone       6\n",
       "29980                                             zonked       1\n",
       "29981                                                zoo       1\n",
       "29982                                               zoom       9\n",
       "29983                                             zoomin       1\n",
       "29984                                          zoophilia       1\n",
       "29985                                             zscout       1\n",
       "29986                                              zsero       1\n",
       "29987                                                 zt       1\n",
       "29988                                                 zu       2\n",
       "29989                                             zubaty       1\n",
       "29990                                            zubrins       1\n",
       "29991                                               zuck      20\n",
       "29992                                         zuckerberg       5\n",
       "29993                                               zuma       2\n",
       "29994                                             zurich       1\n",
       "29995                                                zus       1\n",
       "29996                                             zyklon       2\n",
       "29997                                             zzuuzz       1\n",
       "29998                                               zzzz       1\n",
       "29999                                            zzzzzzz       1\n",
       "\n",
       "[30000 rows x 2 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_df.rename(columns = {0: 'word'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=30000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=True,\n",
       "        token_pattern='\\\\w{1,}', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is kept for heroku purposes\n",
    "train_features = train_word_features\n",
    "test_features = test_word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on toxic set: 0.96308\n",
      "CV score for class toxic is 0.9720911734100458\n",
      "Accuracy of logistic regression classifier on severe_toxic set: 0.99131\n",
      "CV score for class severe_toxic is 0.9850439260374024\n",
      "Accuracy of logistic regression classifier on obscene set: 0.97990\n",
      "CV score for class obscene is 0.9849794924921239\n",
      "Accuracy of logistic regression classifier on threat set: 0.99737\n",
      "CV score for class threat is 0.9860592598384071\n",
      "Accuracy of logistic regression classifier on insult set: 0.97404\n",
      "CV score for class insult is 0.9779611194696516\n",
      "Accuracy of logistic regression classifier on identity_hate set: 0.99249\n",
      "CV score for class identity_hate is 0.9755127187544805\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "log_predictions = {'id': test['id']}\n",
    "log_models = {}\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    log_classifier = LogisticRegression(solver='sag')\n",
    "    log_classifier.fit(train_features, train_target)\n",
    "    \n",
    "    print('Accuracy of logistic regression classifier on {} set: {:.5f}'.format(class_name,log_classifier.score(train_features, train_target)))\n",
    "    \n",
    "    cv_loss = np.mean(cross_val_score(log_classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "    \n",
    "    log_models[class_name] = log_classifier\n",
    "    log_predictions[class_name] = log_classifier.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle the models\n",
    "# Save Model as a pickle Using joblib\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "  \n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(log_models, 'Logistic_Regression_models.p')\n",
    "pickle.dump(train_char_features, open(\"train_char_features_vectorizer.p\", \"wb\"))\n",
    "pickle.dump(test_char_features, open(\"test_char_features_vectorizer.p\", \"wb\"))\n",
    "pickle.dump(word_vectorizer.fit(all_text), open(\"log_word_vectorizer.p\", \"wb\"))\n",
    "\n",
    "  \n",
    "# Load the model from the file \n",
    "# pickled_models = joblib.load('models.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9580769024062893\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b43e0c3612c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0metc_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0metc_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0metc_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "losses = []\n",
    "etc_predictions = {'id': test['id']}\n",
    "etc_models = {}\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    etc_classifier = ExtraTreesClassifier(n_estimators=30)\n",
    "    \n",
    "    cv_loss = np.mean(cross_val_score(etc_classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_loss))\n",
    "    \n",
    "    etc_classifier.fit(train_features, train_target)\n",
    "    etc_models[class_name] = etc_classifier\n",
    "    etc_predictions[class_name] = etc_classifier.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle the models\n",
    "# Save Model as a pickle Using joblib\n",
    "# Save the model as a pickle in a file \n",
    "joblib.dump(etc_models, 'etc_models.p') \n",
    "  \n",
    "# Load the model from the file \n",
    "pickled_models = joblib.load('etc_models.p')  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_models['toxic'].fit(train_features, train_target)\n",
    "predictions['toxic'] = pickled_models['toxic'].predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = pickled_models['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_loss = np.mean(cross_val_score(toxic, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "print('CV score for toxic class is {}'.format(cv_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Code (Unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _train_model(train_x, test_features):\n",
    "#     predictions = {'id': test['id']}\n",
    "#     for class_name in class_names:\n",
    "#         train_target = train[class_name]\n",
    "#         classifier = LogisticRegression(solver='sag')\n",
    "#         classifier.fit(train_X, train_y)\n",
    "#         predictions[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "#     return predictions\n",
    "\n",
    "# def train_folds(X, y, fold_count, test_features):\n",
    "#     fold_size = len(X) // fold_count\n",
    "#     all_predections = []\n",
    "#     for fold_id in range(0, fold_count):\n",
    "#         fold_start = fold_size * fold_id\n",
    "#         fold_end = fold_start + fold_size\n",
    "\n",
    "#         if fold_id == fold_size - 1:\n",
    "#             fold_end = len(X)\n",
    "\n",
    "#         train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "#         train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "#         val_x = X[fold_start:fold_end]\n",
    "#         val_y = y[fold_start:fold_end]\n",
    "    \n",
    "#         print(\"In fold #\", fold_id)\n",
    "#         all_predections.append(_train_model(train_x, train_y))\n",
    "#     return all_predections\n",
    "\n",
    "# train_folds(train_features, test_features, train_features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# submission = pd.DataFrame.from_dict(predictions)\n",
    "# submission.to_csv('Logistic-Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup nltk corpora path and Google Word2Vec location\n",
    "google_vec_file = '/Users/randy/Documents/GitHub/Twitch_Chat_Harassment/notebooks/GoogleNews-vectors-negative300.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138045430183411),\n",
       " ('queen', 0.6510956883430481),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('king' ,topn=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6641711"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity(['king', 'man'], ['queen', 'woman'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
